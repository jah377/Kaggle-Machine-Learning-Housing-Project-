KAGGLE ML PROJECT: READ ME
By: Jon Harris

# Introduction
The following document summarizes the overall procedure of data wrangling, imputation, feature engineering, and modeling necessary for the Kaggle machine learning project at the NYC Data Science Academy. 

Our approach for the project was that of house flippers - individuals that buy a house and renovate key features with the hope of selling the house at a higher price. As such, developing ML models and feature selection is necessary to ensure that we are renovating key house features that will maximally contribute to the price of the house. 

The following pipeline was implemented for our data processing: 
	1. Import raw data
	2. Identify and remove outliers
	3. Impute data
	4. Feature engineer
	5. Dummify categorical variables  


# ===========================================================================
# 							Data Imputation
# ===========================================================================

Initial counts of missing data [pd.DataFrame.info()] may be inflated as several features use NA to indicate absence of said feature (BsmtQual: NA = No Basement). 

Actual missing values were handled based on categorical and numerical data.
