1. MLR
	- Model details:
		- Original dataset
		- Use all variables
		- Test-Train to 
	- Show:
		- Overfit, not good model
		- Multicollinearity a problem

2. MLR + Forward-Backward Selection + VIF
	- Model details:
		- Original dataset
		- FB Selection to identify useful variables
		- Perform VIF to remove highly collinear variables?
	- Show:
		- Feature selection improved accuracy
		- Minimizing collinearity improved accuracy
		- Multicollinearity may still be a problem, so we should do regression
3. LASSO
	- Model details:
		- Original dataset
		- all variables
		- k-fold to tune lambda
	- Show:
		- Regularized regressiom improved accuracy
4. LASSO + Feature Engineering
	- Model details:
		- Engineered dataset
		- k-fold to tune lambda
	- Show:
		- Regularized regression useful
		- Feature engineering *was* helpful
5. LASSO + Feature Engineering + Independent Variable Transform
	- Model details:
		- Engineered dataset
		- k-fold to tune lambda
	- Show:
		- Show skewness of independent continuous variables may affect accuracy
6. Ridge + Feature Engineering + Independent Variable Transform
	- Model details:
		- Engineered dataset
		- k-fold to tune lambda + rho
	- Show:
		- ElasticNet may or may not improve accuracy
		- Model limited by assumption of linear relationship
7. XGBoost + Feature Engineering
	- Model details:
		- Engineered dataset
		- Tune hyperparameters
	- Show:
		- XGBoost may or may not improve accuracy
